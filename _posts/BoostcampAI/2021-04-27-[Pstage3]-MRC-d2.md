---
title: "[Pstage3] Day2. MRC"
subtitle: 기계독해, Extraction-based MRC, Generation-based MRC
tags: [기계독해, MRC, NLP] 

---

Boostcamp Pstage_03 Day 02. 2021-04-27.  
DAYLY LOG

# Open-Domain Question & Answring

### Contents
- 목표/행동/회고

<br>

## 1. 오늘의 학습목표
- [Introducing BART](https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html) 읽기
- [Neural Machine Reading Comprehension:Methods and Trends](https://arxiv.org/pdf/1907.01118.pdf) 1강 수업 Reference 부분 읽기.
- SQuAD 데이터셋 둘러보기
- 문자열 type에 관련된 정리글 읽기 ([Encodings and Character Sets to work with Text](https://kunststube.net/encoding/))

<br>

## 2. 무엇을 (학습, 구현) 했는가

## 2-1. Introducing BART
As the BART authors write,
> (BART) can be seen as generalizing Bert(due to the bidirectional encoder) and GPT2(with the left to right decoder.)
- `Bert` : Fully-Visible Mask  
    Bert는 Masked Tokens를 예측 하도록 Pretrained된 모델이며,
    그리고 문장 전체를 학습하기 때문에 좋은 예측을 할 수 있는 충분한 정보를 얻는다.  

    - 다음 문장 예측하는건 잘함
    - 다만 다음 문장 생성 TASK는 잘 못함
        - 오직 이전 생성된 단어에 의존적이기 때문

- `GPT` : Causal Mask  
    인과관계 마스크(Causal Mask)를 사용해서 다음 단어를 예측하도록 Pretrained 되었으며, 
    - Generation tasks에 효과적이고 
    - Downstream tasks에는 덜 효과적
        - fine-tuning해서 사용하는게 별로라는 뜻인가(?)

- `Encoder-Decoder`  
    근데 `BART`는 위에 두개 다 잘 한단다.  
    > - Encoder의 `attention_mask`는 fully visible하다. 마치 BERT처럼
    > - Decoder의 `attention_mask`는 causal하다. 마치 GPT처럼

    - encoder와 decoder는 서로 cross-attention으로 연결되어 있고, encoder의 output의 마지막 최종 hidden state에 대해서 각각 decoder layer가 attention over를 수행한다.  
    
    - 이러는 이유는 아마 원래 입력과 밀접하게 연결된 출력을 생성하기 위해서 이고, 이를 위해 모델을 조금씩 움직인다.



<br>

## 3. 아쉬운점



<br>

## 내일 할 것

- BaseLine Code 숙지하기
- PyCharm SSH Remote 설정하기
- 남은 논문 마저 읽기
- Daily Mission 3,4 하기





<br><br>



## Reference

- bootcamp AI Tech pdf.  
- NAVER Connect Foundation.

