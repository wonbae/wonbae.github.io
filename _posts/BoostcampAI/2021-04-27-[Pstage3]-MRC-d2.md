---
title: "[Pstage3] Day2. MRC"
subtitle: 기계독해, Extraction-based MRC, Generation-based MRC
tags: [기계독해, MRC, NLP] 

---

Boostcamp Pstage_03 Day 02. 2021-04-27.  
DAYLY LOG

# Open-Domain Question & Answring

### Contents
- 목표/행동/회고

<br>

## 1. 오늘의 학습목표
- [Introducing BART](https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html) 읽기
- [Neural Machine Reading Comprehension:Methods and Trends](https://arxiv.org/pdf/1907.01118.pdf) 1강 수업 Reference 부분 읽기.
- SQuAD 데이터셋 둘러보기
- 문자열 type에 관련된 정리글 읽기 ([Encodings and Character Sets to work with Text](https://kunststube.net/encoding/))

<br>

## 2. 무엇을 (학습, 구현) 했는가

## 2-1. Introducing BART
As the BART authors write,
> (BART) can be seen as generalizing Bert(due to the bidirectional encoder) and GPT2(with the left to right decoder.)
- `Bert` : Fully-Visible Mask  
    Bert는 Masked Tokens를 예측 하도록 Pretrained된 모델이며,
    그리고 문장 전체를 학습하기 때문에 좋은 예측을 할 수 있는 충분한 정보를 얻는다.  

    - 다음 문장 예측하는건 잘함
    - 다만 다음 문장 생성 TASK는 잘 못함
        - 오직 이전 생성된 단어에 의존적이기 때문

- `GPT` : Causal Mask  
    인과관계 마스크(Causal Mask)를 사용해서 다음 단어를 예측하도록 Pretrained 되었으며, 
    - Generation tasks에 효과적이고 
    - Downstream tasks에는 덜 효과적
        - fine-tuning해서 사용하는게 별로라는 뜻인가(?)

- `Encoder-Decoder`  
    근데 `BART`는 위에 두개 다 잘 한단다.  
    > - Encoder의 `attention_mask`는 fully visible하다. 마치 BERT처럼
    > - Decoder의 `attention_mask`는 causal하다. 마치 GPT처럼

    - encoder와 decoder는 서로 cross-attention으로 연결되어 있고, encoder의 output의 마지막 최종 hidden state에 대해서 각각 decoder layer가 attention over를 수행한다.  

    - 이러는 이유는 아마 원래 입력과 밀접하게 연결된 출력을 생성하기 위해서 이고, 이를 위해 모델을 조금씩 움직인다.

## 2-2. SQuAD, KorQuAD DataSet

> *<i class="fa fa-info-circle" aria-hidden="true"></i> 정보:* 두 데이터셋 모두 Exact Match(EM) 및 F1 Score를 기준으로 모델의 성능을 측정한다.

- `S`tanford `Q`uestion `A`nswering `D`ataset (`SQuAD`) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

- `SQuAD2.0` combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.

- `SQuAD 1.1`, the previous version of the SQuAD dataset, contains 100,000+ question-answer pairs on 500+ articles.

- `KorQuAD 2.0`은 KorQuAD 1.0에서 질문답변 20,000+ 쌍을 포함하여 총 100,000+ 쌍으로 구성된 한국어 Machine Reading Comprehension 데이터셋 입니다. *KorQuAD 1.0과는 다르게 1~2 문단이 아닌 Wikipedia article 전체에서 답을 찾아야 합니다.* 매우 긴 문서들이 있기 때문에 탐색 시간에 대한 고려가 필요할 것 입니다. 또한 표와 리스트도 포함되어 있기 때문에 HTML tag를 통한 문서의 구조 이해도 필요합니다. 이 데이터셋을 통해서 다양한 형태와 길이의 문서들에서도 기계독해가 가능해질 것 입니다.

- `KorQuAD 2.1`의 전체 데이터는 47,957 개의 Wikipedia article에 대해 102,960 개의 질의응답 쌍으로, Training set 83,486 개, Dev set 10,165 개의 질의응답쌍으로 구분하였습니다. KorQuAD 2.0 데이터 중 HTML 태그의 속성이 완벽하게 제거되지 않은 오류를 수정하여 재배포한 데이터셋으로, KorQuAD 2.0의 태그 속성을 제외한 원본과 정답 텍스트가 바뀌는 경우는 없습니다.
KorQuAD 2.0의 데이터셋은 `CC BY-ND 2.0 KR` 라이센스를 따릅니다.

<!-- > **<i class="fa fa-exclamation-triangle" aria-hidden="true"></i> 주의:** 이 구문은 마크다운 인용구 문법 입니다. -->



> **<i class="fa fa-question-circle"></i> 질문 : SQuAD의 리더보드는 Ensemble model이 상위권이고 대다수인데 KorQuAD는 왜 전부 Single Model일까?**   
> 궁금하다.


## 2-3. Peer-Session
- 실습 코드에 대한 이야기
- 우리의 컴퍼티션은 `Extraction-based MRC`일까`Generation-based MRC`일까?
    - 둘다라고 생각. MRC이지만, Open-Domain QA이기 때문에 지문에 없는 답변을 해야할수도 있고 질문의 ENTITY가 지문에 없을 수도 있기 때문에 기본으로 Extract를 시도하고 $+ \alpha$로 Generte를 생각해야 할듯.
- Colab에서 directory를 이동할때, ```!cd /content/drive/MyDrive/some_file``` 이런식으로 하면 일시적으로만 실행되지 지속적으로 적용되지 않는다. !대신에 ```%cd /content/drive/MyDrive/some_file```이런식으로 %를 사용해야 적용된다는걸 알음.  

- 그 외. 어려 이야기들.



<br>

## 3. 아쉬운점



<br>

## 내일 할 것

- 오늘 논문 읽은거 간단하게 나마 여기에 정리해서 올리기
- BaseLine Code 숙지하기
- PyCharm SSH Remote 설정하기
- 남은 논문 마저 읽기
- Daily Mission 3,4 하기


<br><br>


## Reference

- bootcamp AI Tech pdf.  
- NAVER Connect Foundation.

